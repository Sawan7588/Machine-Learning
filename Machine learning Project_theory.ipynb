{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd485a8",
   "metadata": {},
   "source": [
    "## Steps in a Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986898ea",
   "metadata": {},
   "source": [
    " - A Machine Learning Project involves the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b0e83",
   "metadata": {},
   "source": [
    "**Defining the Problem:**\n",
    "    \n",
    " - Define a problem statement, which addresses a business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00737e",
   "metadata": {},
   "source": [
    "**Obtaining the Source Data:**\n",
    "    \n",
    " - The raw data required to build a model can be presented in a single or multiple sources such as relational databases, and social networking sites.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c760df",
   "metadata": {},
   "source": [
    "**Understanding Data Through Visualization:**\n",
    " - Look into data and understand important features such as its mean, and spread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ebf9b",
   "metadata": {},
   "source": [
    "**Preparing Data for Machine Learning Algorithms:**\n",
    " - Mostly, the captured raw data cannot be used to train using a Machine learning algorithm. The raw datasets have to be manipulated or transformed through one or more pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838689c",
   "metadata": {},
   "source": [
    "**Choosing an algorithm:**\n",
    " - Based on features of data set, pick a suitable algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eea2da",
   "metadata": {},
   "source": [
    "**Building the Model:**\n",
    " - Train the algorithm with considered training data set and verify its performance through a metric.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398249df",
   "metadata": {},
   "source": [
    "**Fine-tuning the Model:**\n",
    " - Identify values of vital parameters, associated with the chosen model for better performance.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688933b",
   "metadata": {},
   "source": [
    "**Use the best model:**\n",
    " - Use the model with better performance for addressing the defined problem.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8693e",
   "metadata": {},
   "source": [
    "## Introduction to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460999ba",
   "metadata": {},
   "source": [
    "scikit-learn is a Machine learning toolkit in Python. The package contains efficient tools used for Data Mining and Data Analysis.\n",
    "\n",
    "It is built on NumPy, SciPy, and matplotlib packages. It is opensource and also commercially usable under BSD license."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61795247",
   "metadata": {},
   "source": [
    "### scikit-learn Utilities\n",
    "- scikit-learn library has many utilities that can be used to perform the following tasks involved in Machine Learning.\n",
    "\n",
    "  - Preprocessing\n",
    "  - Model Selection\n",
    "  - Classification\n",
    "  - Regression\n",
    "  - Clustering\n",
    "  - Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc29cf7",
   "metadata": {},
   "source": [
    "### Steps with scikit-learn\n",
    " - Mostly, one would perform the following steps while working on a Machine learning problem with scikit-learn:\n",
    "\n",
    " 1. Cleaning raw data set.\n",
    " 2. Further transforming with many scikit-learn pre-processing utilities.\n",
    " 3. Splitting data into train and test sets with train_test_split utility.\n",
    " 4. Creating a suitable model with default parameters.\n",
    " 5. Training the Model using fit function.\n",
    " 6. Evaluating the Model and fine-tuning it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cda18",
   "metadata": {},
   "source": [
    "### Reading Data for ML\n",
    " - Any Machine Learning Algorithm requires data for building a model.\n",
    "\n",
    " - The data can be obtained from Multiple sources such as http, ftp repositories, databases, local repositories, etc.\n",
    "\n",
    " - Many times raw data, read from a source, cannot be used directly by an ML algorithm for building a Model.\n",
    "\n",
    " - So, raw data has to be cleaned, processed, transformed (if required) and then passed to an ML algorithm always."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91dea9",
   "metadata": {},
   "source": [
    "### Example Data - Breast Cancer Dataset\n",
    " - Breast Cancer data set is a popular one, which contains details of 30 features obtained from 569 cancer patients.\n",
    "\n",
    " - We will be doing the following tasks and make cancer data set ready for ML.\n",
    "\n",
    "  - Reading raw data from UCI archive\n",
    "  - Extract features from Raw data.\n",
    "  - Naming or Labelling features\n",
    "  - Extract target values from Raw data\n",
    "  - Naming or Labelling target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee42e3",
   "metadata": {},
   "source": [
    "#### Reading Data from UCI Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76ef10",
   "metadata": {},
   "source": [
    "##### The raw data set from UCI archive can be read with the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1368f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_set = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', \n",
    "                        header = None)\n",
    "print(cancer_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457d454",
   "metadata": {},
   "source": [
    " - Read raw dataset contains 32 columns.\n",
    " - the 1st column has patient ID details, and the 2nd one has tumor type, i.e. malignant or benign.\n",
    " - The rest 30 columns represent various features obtained from each patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a257d7",
   "metadata": {},
   "source": [
    "##### Extracting Features from Raw Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8773f84",
   "metadata": {},
   "source": [
    " - All columns, representing features are extracted with the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f1dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "cancer_features = cancer_set.iloc[:,2:]\n",
    "\n",
    "print(cancer_features.shape)\n",
    "print(type(cancer_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b9ac6",
   "metadata": {},
   "source": [
    " - cancer_features is a dataframe. It is converted to a numpy array with below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a693eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "cancer_features = cancer_features.values\n",
    "print(cancer_features.shape)\n",
    "print(type(cancer_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9ff4e",
   "metadata": {},
   "source": [
    "#### Naming features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8077da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_features_names = ['mean radius', \n",
    "'mean texture', 'mean perimeter', \n",
    "'mean area', 'mean smoothness', \n",
    "'mean compactness', 'mean concavity',\n",
    "'mean concave points', 'mean symmetry',\n",
    "'mean fractal dimension','radius error',\n",
    "'texture error','perimeter error',\n",
    "'area error', 'smoothness error',\n",
    "'compactness error','concavity error',\n",
    "'concave points error','symmetry error',\n",
    "'fractal dimension error','worst radius',\n",
    "'worst texture', 'worst perimeter', \n",
    "'worst area','worst smoothness', \n",
    "'worst compactness', 'worst concavity',\n",
    "'worst concave points','worst symmetry',\n",
    "'worst fractal dimension']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f70076b",
   "metadata": {},
   "source": [
    "#### Extracting target values from Raw data\n",
    " - Target values of each patient are extracted with below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e114ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "cancer_target = cancer_set.iloc[:, 1]\n",
    "\n",
    "# Replacing 'M' with 0 and 'B' with 1\n",
    "cancer_target = cancer_target.replace(['M', 'B'], [0, 1])\n",
    "\n",
    "# Converting to numpy array\n",
    "cancer_target = cancer_target.values\n",
    "\n",
    "print(type(cancer_target))\n",
    "print(cancer_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e401f9",
   "metadata": {},
   "source": [
    "### scikit-learn Datasets\n",
    " - scikit-learn by default comes with few popular datasets.\n",
    "\n",
    " - They can be loaded into your working environment and used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8853262",
   "metadata": {},
   "source": [
    "### Reading Cancer Data from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384b9301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "print(cancer.data.shape)\n",
    "print(cancer.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cea594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(type(iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b12b8",
   "metadata": {},
   "source": [
    "## Preprocessing - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae8ebd",
   "metadata": {},
   "source": [
    "**Preprocessing is a step, in which raw data is modified or transformed into a format, suitable for further downstream processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8071d",
   "metadata": {},
   "source": [
    "scikit-learn provides many preprocessing utilities such as,\n",
    "\n",
    " - Standardization mean removal\n",
    " - Scaling\n",
    " - Normalization\n",
    " - Binarization\n",
    " - One Hot Encoding\n",
    " - Label Encoding\n",
    " - Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888bfe2",
   "metadata": {},
   "source": [
    "### 1 . Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c249d",
   "metadata": {},
   "source": [
    "#### Standardization or Mean Removal is the process of transforming each feature vector into a normal distribution with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce6500",
   "metadata": {},
   "source": [
    " - This can be achieved using StandardScaler.\n",
    " - An example with its output is shown in the next two cards, which requires the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8f98d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of each feature after Standardization : \n",
      "\n",
      "\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "\n",
      "Std. of each feature after Standardization : \n",
      "\n",
      "\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "standardizer  = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(breast_cancer.data)\n",
    "breast_cancer_standardizer = standardizer.transform(breast_cancer.data)\n",
    "\n",
    "print(\"Mean of each feature after Standardization : \\n\\n\")\n",
    "print(breast_cancer_standardizer.mean(axis=0))\n",
    "print(\"\\nStd. of each feature after Standardization : \\n\\n\")\n",
    "print(breast_cancer_standardizer.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb66d5d",
   "metadata": {},
   "source": [
    " - Axis is used to compute the means and standard deviations along the given data.\n",
    "   - If 0, independently standardize each feature,\n",
    "   - otherwise (if 1) standardize each sample.\n",
    " - “axis 0” represents rows and “axis 1” represents columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b0025",
   "metadata": {},
   "source": [
    "### 2. Scaling\n",
    "\n",
    "\n",
    "Scaling transforms existing data values to lie between a minimum and maximum value.\n",
    "  - MinMaxScaler transforms data to range 0 and 1.\n",
    "  - MaxAbsScaler transforms data to range -1 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3019a7",
   "metadata": {},
   "source": [
    "#### 2.1 Using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16857b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler  = preprocessing.MinMaxScaler().fit(breast_cancer.data)\n",
    "breast_cancer_minmaxscaled = min_max_scaler.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a2b77",
   "metadata": {},
   "source": [
    "**By default, transformation occurs to a range of 0 and 1. It can also be customized with feature_range argument as shown in next example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047aff1f",
   "metadata": {},
   "source": [
    " - MinMaxScaler with specified range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81ce15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,10)).fit(breast_cancer.data)\n",
    "breast_cancer_minmaxscaled10 = min_max_scaler.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a209b9",
   "metadata": {},
   "source": [
    "#### In the above example, data is transformed to range 0 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc06745",
   "metadata": {},
   "source": [
    "#### 2.2 Using MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e5983",
   "metadata": {},
   "source": [
    " - Using MaxAbsScaler, the maximum absolute value of each feature is scaled to unit size, i.e., 1. It is intended for data that is previously centered at sparse or zero data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a84a68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler().fit(breast_cancer.data)\n",
    "\n",
    "breast_cancer_maxabsscaled = max_abs_scaler.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b4959",
   "metadata": {},
   "source": [
    "#### By default, MaxAbsScaler transforms data to the range -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e9dc9",
   "metadata": {},
   "source": [
    "### 3. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ba4fc",
   "metadata": {},
   "source": [
    " - Normalization scales each sample to have a unit norm.\n",
    " - Normalization can be achieved with 'l1', 'l2', and 'max' norms.\n",
    " - 'l1' norm makes the sum of absolute values of each row as 1, and 'l2' norm makes the sum of squares of each row as 1.\n",
    " - 'l1' norm is insensitive to outliers.\n",
    " - By default l2 norm is considered. Hence, removing outliers is recommended before applying l2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1849b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normlizer = preprocessing.Normalizer(norm='l1').fit(breast_cancer.data)\n",
    "breast_cancer_normlized = normlizer.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ddc105",
   "metadata": {},
   "source": [
    "### 4. Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4cd42",
   "metadata": {},
   "source": [
    "Binarization is the process of transforming data points to 0 or 1 based on a given threshold.\n",
    " - Any value above the threshold is transformed to 1, and any value below the threshold is transformed to 0\n",
    " - By default, a threshold of 0 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b54d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "binarizer  = preprocessing.Binarizer(threshold=3.0).fit(breast_cancer.data)\n",
    "breast_cancer_binarized = binarizer.transform(breast_cancer.data)\n",
    "print(breast_cancer_binarized[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5623252",
   "metadata": {},
   "source": [
    "### 5. OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6a903",
   "metadata": {},
   "source": [
    " - OneHotEncoder converts categorical integer values into one-hot vectors. In an one-hot vector, every category is transformed into a binary attribute having only 0 and 1 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34aa35e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]]\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "onehotencoder = preprocessing.OneHotEncoder()\n",
    "onehotencoder = onehotencoder.fit([[1], [1], [1], [2], [2], [1]])\n",
    "\n",
    "# Transforming category values 1 and 2 to one-hot vectors\n",
    "print(onehotencoder.transform([[1]]).toarray())\n",
    "print(onehotencoder.transform([[2]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a1205",
   "metadata": {},
   "source": [
    "### 6. Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97530e",
   "metadata": {},
   "source": [
    " - Label Encoding is a step in which, in which categorical features are represented as categorical integers. An example of transforming categorical values [\"benign\",\"malignant\"]into[0, 1]` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75038423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "labels = ['malignant', 'benign', 'malignant', 'benign']\n",
    "\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "\n",
    "labelencoder = labelencoder.fit(labels)\n",
    "\n",
    "bc_labelencoded = labelencoder.transform(breast_cancer.target_names)\n",
    "\n",
    "print(bc_labelencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0d87b",
   "metadata": {},
   "source": [
    "### 7. Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870e6e0",
   "metadata": {},
   "source": [
    " - Imputation replaces missing values with either median, mean, or the most common value of the column or row in which the missing values exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e557bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values='NaN', strategy='mean')\n",
    "\n",
    "imputer = imputer.fit(breast_cancer.data)\n",
    "breast_cancer_imputed = imputer.transform(breast_cancer.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae550c2",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b870d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75140029 0.40517418 0.45478362 0.14107142]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[6.262 2.872 4.906 1.676]\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "# Task 1\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "iris = datasets.load_iris()\n",
    "normalizer = preprocessing.Normalizer(norm='l2').fit(iris.data)\n",
    "iris_normalized = normalizer.transform(iris.data)\n",
    "print(iris_normalized.mean(axis=0))\n",
    "\n",
    "# Task 2\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "iris_target_onehot = enc.fit_transform(iris.target.reshape(-1, 1))\n",
    "print(iris_target_onehot.toarray()[[0,50,100]])\n",
    "\n",
    "# Task 3\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "iris.data[:50, :] = np.nan\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "iris_imputed = imputer.fit_transform(iris.data)\n",
    "\n",
    "print(iris_imputed.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7020e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "regions = ['HYD', 'CHN', 'MUM', 'HYD', 'KOL', 'CHN']\n",
    "print(preprocessing.LabelEncoder().fit(regions).transform(regions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0bf6bd",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e8b8b",
   "metadata": {},
   "source": [
    "Nearest neighbors method is used to determine a predefined number of data points that are closer to a sample point and predict its label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf193ee",
   "metadata": {},
   "source": [
    " - sklearn.neighbors provides utilities for unsupervised and supervised neighbors-based learning methods.\n",
    " - scikit-learn implements two different nearest neighbors classifiers:\n",
    "     - KNeighborsClassifier\n",
    "     - RadiusNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec808b9c",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b677696",
   "metadata": {},
   "source": [
    " - KNeighborsClassifier classifies based on k nearest neighbors of every query point, where k is an integer value specified by the user.\n",
    "\n",
    " - RadiusNeighborsClassifier classifies based on the number of neighbors present in a fixed radius r of every training point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069dbb7",
   "metadata": {},
   "source": [
    "### Nearest Neighbors Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887128ff",
   "metadata": {},
   "source": [
    "scikit-learn implements the following two regressors:\n",
    "\n",
    " - KNeighborsRegressor predicts based on the k nearest neighbors of each query point.\n",
    " - RadiusNeighborsRegressor predicts based on the neighbors present in a fixed radius r of the query point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5f7d8",
   "metadata": {},
   "source": [
    "### Demo of KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20240c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "## Building a Model of KNN classifier\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                   stratify=cancer.target, random_state=42)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier = knn_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38b8e6",
   "metadata": {},
   "source": [
    "### Determining Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a94cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train data :  0.9460093896713615\n",
      "\n",
      "Accuracy of test data :  0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of train data : \", knn_classifier.score(X_train,Y_train))\n",
    "print(\"\\nAccuracy of test data : \", knn_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4f4d2",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0c12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "Accuracy on train dataset :  0.9821428571428571\n",
      "Accuracy on test dataset :  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target,\n",
    "                                                   stratify=iris.target, random_state=30)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Task 2\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf = knn_clf.fit(X_train,Y_train)\n",
    "print(\"Accuracy on train dataset : \", knn_clf.score(X_train,Y_train))\n",
    "print(\"Accuracy on test dataset : \", knn_clf.score(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde0bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for different n_neighbors values:\n",
      "n_neighbors = 3: accuracy = 1.000\n",
      "n_neighbors = 4: accuracy = 1.000\n",
      "n_neighbors = 5: accuracy = 1.000\n",
      "n_neighbors = 6: accuracy = 1.000\n",
      "n_neighbors = 7: accuracy = 0.967\n",
      "n_neighbors = 8: accuracy = 1.000\n",
      "n_neighbors = 9: accuracy = 1.000\n",
      "n_neighbors = 10: accuracy = 1.000\n",
      "The model with n_neighbors = 3 had the highest accuracy score of 1.000.\n",
      "The model with n_neighbors 3 had the highest accuracy score of 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the range of n_neighbors values to try\n",
    "n_neighbors_range = range(3, 11)\n",
    "\n",
    "# Initialize a dictionary to store the accuracy scores for each model\n",
    "accuracy_scores = {}\n",
    "\n",
    "# Loop over the n_neighbors values and fit a KNN model for each\n",
    "for n_neighbors in n_neighbors_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    accuracy_scores[n_neighbors] = knn.score(X_test, Y_test)\n",
    "\n",
    "# Find the n_neighbors value with the highest accuracy score\n",
    "best_n_neighbors = max(accuracy_scores, key=accuracy_scores.get)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy scores for different n_neighbors values:\")\n",
    "for n_neighbors, accuracy in accuracy_scores.items():\n",
    "    print(f\"n_neighbors = {n_neighbors}: accuracy = {accuracy:.3f}\")\n",
    "    \n",
    "print(f\"The model with n_neighbors = {best_n_neighbors} had the highest accuracy score of {accuracy_scores[best_n_neighbors]:.3f}.\")\n",
    "print(\"The model with n_neighbors\", best_n_neighbors, \"had the highest accuracy score of\", accuracy_scores[best_n_neighbors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "571487dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "Accuracy on train dataset :  0.9821428571428571\n",
      "Accuracy on test dataset :  0.9473684210526315\n",
      "Accuracy scores for different n_neighbors values:\n",
      "n_neighbors = 3: accuracy = 0.947\n",
      "n_neighbors = 4: accuracy = 0.947\n",
      "n_neighbors = 5: accuracy = 0.947\n",
      "n_neighbors = 6: accuracy = 0.974\n",
      "n_neighbors = 7: accuracy = 0.947\n",
      "n_neighbors = 8: accuracy = 0.947\n",
      "n_neighbors = 9: accuracy = 0.947\n",
      "n_neighbors = 10: accuracy = 0.921\n",
      "The model with n_neighbors = 6 had the highest accuracy score of 0.9736842105263158.\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "# Task 1\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target,\n",
    "                                                   stratify=iris.target, random_state=30)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Task 2\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf = knn_clf.fit(X_train,Y_train)\n",
    "print(\"Accuracy on train dataset : \", knn_clf.score(X_train,Y_train))\n",
    "print(\"Accuracy on test dataset : \", knn_clf.score(X_test,Y_test))\n",
    "\n",
    "# Task 3\n",
    "\n",
    "# Define the range of n_neighbors values to try\n",
    "n_neighbors_range = range(3, 11)\n",
    "\n",
    "# Initialize a dictionary to store the accuracy scores for each model\n",
    "accuracy_scores = {}\n",
    "\n",
    "# Loop over the n_neighbors values and fit a KNN model for each\n",
    "for n_neighbors in n_neighbors_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    accuracy_scores[n_neighbors] = knn.score(X_test, Y_test)\n",
    "\n",
    "# Find the n_neighbors value with the highest accuracy score\n",
    "best_n_neighbors = max(accuracy_scores, key=accuracy_scores.get)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy scores for different n_neighbors values:\")\n",
    "for n_neighbors, accuracy in accuracy_scores.items():\n",
    "    print(\"n_neighbors = {}: accuracy = {:.3f}\".format(n_neighbors, accuracy))\n",
    "    \n",
    "\n",
    "print(\"The model with n_neighbors = \" + str(best_n_neighbors) + \" had the highest accuracy score of \" + str(accuracy_scores[best_n_neighbors]) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68641dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "split iris dataset into two sets names X_train and X_test also split iris.target into two sets Y_train and Y_test\n",
    "Hint: use train_test_slit method from sklearn.model_selection, set random_state to 30 and perform stratified sampling\n",
    "Print shape of X_train dataset\n",
    "Print shape of X_test dataset\n",
    "fit k nearest neighbors model on X_train data and Y_train labels with default parameters Name the model as knn_clf.\n",
    "Evaluate the  model accuracy on training dataset and print it's score\n",
    "Evaluate the  model accuracy on testing dataset and print it's score\n",
    "fit multiple k nearest neighbors models on X_train data and Y_train labels with n_neighbors  parameter value changing from 3 to 10\n",
    "evaluate each model accuracy on testing dataset Hint make use of for loop\n",
    "print the n_neighbors value of the model with highest accuracy use iris data from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ed7d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (120, 4)\n",
      "Shape of X_test: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and testing sets using stratified sampling\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=30, stratify=iris.target)\n",
    "\n",
    "# Print the shape of the training and testing sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit k nearest neighbors model on X_train data and Y_train labels with default parameters Name the model as knn_clf.\n",
    "Evaluate the  model accuracy on training dataset and print it's score\n",
    "Evaluate the  model accuracy on testing dataset and print it's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b66c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "0.9821428571428571\n",
      "0.9473684210526315\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, stratify=iris.target, random_state=30)\n",
    "\n",
    "# Print the shape of the training and testing sets\n",
    "print( X_train.shape)\n",
    "print( X_test.shape)\n",
    "\n",
    "# Fit a K-Nearest Neighbors model on the training data\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model on the training and testing data\n",
    "train_acc = knn_clf.score(X_train, Y_train)\n",
    "test_acc = knn_clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_acc)\n",
    "print(test_acc)\n",
    "\n",
    "# Fit multiple K-Nearest Neighbors models with different n_neighbors values and evaluate them on the testing data\n",
    "best_acc = 0\n",
    "best_k = 0\n",
    "for k in range(3, 11):\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_clf.fit(X_train, Y_train)\n",
    "    acc = knn_clf.score(X_test, Y_test)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_k = k\n",
    "    \n",
    "\n",
    "print(best_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abca503",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938558",
   "metadata": {},
   "source": [
    " Decision Trees is another Supervised Learning method used for Classification and Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2efc1f3",
   "metadata": {},
   "source": [
    " - Decision Trees learn simple decision rules from training data and build a Model.\n",
    "\n",
    " - DecisionTreeClassifier and DecisionTreeRegressor are the two utilities from sklearn.tree, which can be used for classification and regression respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e818f8",
   "metadata": {},
   "source": [
    "### Advantages of Decision Trees\n",
    "Advantages\n",
    "\n",
    " - Decision Trees are easy to understand.\n",
    " - They often do not require any preprocessing.\n",
    " - Decision Trees can learn from both numerical and categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162a6f7",
   "metadata": {},
   "source": [
    "### Disadvantages of Decision Trees\n",
    " - Decision trees sometimes become complex, which do not generalize well and leads to overfitting. Overfitting can be addressed by placing the least number of samples needed at a leaf node or placing the highest depth of the tree.\n",
    "\n",
    " - A small variation in data can result in a completely different tree. This problem can be addressed by using decision trees within an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6525b",
   "metadata": {},
   "source": [
    "### Building a Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16e5f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train dataset :  1.0\n",
      "Accuracy of test dataset :  0.916083916083916\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                   stratify=cancer.target, random_state=42)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier = dt_classifier.fit(X_train,Y_train)\n",
    "\n",
    "print(\"Accuracy of train dataset : \", dt_classifier.score(X_train,Y_train))\n",
    "print(\"Accuracy of test dataset : \", dt_classifier.score(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70885eb",
   "metadata": {},
   "source": [
    "### Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b08003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train dataset : 0.9577464788732394\n",
      "Accuracy of test dataset : 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=2)\n",
    "dt_classifier = dt_classifier.fit(X_train,Y_train)\n",
    "print(\"Accuracy of train dataset :\", dt_classifier.score(X_train,Y_train))\n",
    "print(\"Accuracy of test dataset :\", dt_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d72291",
   "metadata": {},
   "source": [
    "## Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1e9c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n",
      "(127, 13)\n",
      "1.0\n",
      "0.8098834820264638\n",
      "[18.2 13.9]\n",
      "0.6876109752166819 2\n",
      "0.6962264524668584 3\n",
      "0.7086640885662667 4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(100)\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target,random_state=30)\n",
    "\n",
    "print(X_train.shape)\n",
    "print( X_test.shape)\n",
    "\n",
    "# Task 2\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg = dt_reg.fit(X_train,Y_train)\n",
    "\n",
    "print(dt_reg.score(X_train,Y_train))\n",
    "print(dt_reg.score(X_test,Y_test))\n",
    "print(dt_reg.predict(X_test[:2]))\n",
    "\n",
    "# Task 3\n",
    "\n",
    "best_acc = 0\n",
    "best_d = 0\n",
    "\n",
    "for d in range(2,6):\n",
    "    dt_reg = DecisionTreeRegressor(max_depth=d)\n",
    "    dt_reg = dt_reg.fit(X_train,Y_train)\n",
    "    acc = dt_reg.score(X_test,Y_test)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_d = d\n",
    "        print(acc,d)\n",
    "\n",
    "print(best_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611e957",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7fc82",
   "metadata": {},
   "source": [
    "Ensemble methods combine predictions of other learning algorithms, to improve the generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ccf9e",
   "metadata": {},
   "source": [
    " - Ensemble methods are two types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b93c9",
   "metadata": {},
   "source": [
    " - Averaging Methods: They build several base estimators independently and finally average their predictions.\n",
    "   - E.g.: Bagging Methods, Forests of randomised trees\n",
    " - Boosting Methods: They build base estimators sequentially and try to reduce the bias of the combined estimator.\n",
    "     - E.g.: Adaboost, Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd33260",
   "metadata": {},
   "source": [
    "### Bagging Methods\n",
    " - Bagging Methods draw random subsets of the original dataset, build an estimator and aggregate individual results to form a final one.\n",
    "\n",
    " - BaggingClassifier and BaggingRegressor are the utilities from sklearn.ensemble to deal with Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64355f",
   "metadata": {},
   "source": [
    "### Randomized Trees\n",
    " - sklearn.ensemble offers two types of algorithms based on randomized trees: Random Forest and Extra randomness algorithms.\n",
    "\n",
    "   - RandomForestClassifier and RandomForestRegressor classes are used to deal with random forests.\n",
    "   - In random forests, each estimator is built from a sample drawn with replacement from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466e3f5",
   "metadata": {},
   "source": [
    "### Randomized Trees\n",
    " - ExtraTreesClassifier and ExtraTreesRegressor classes are used to deal with extremely randomized forests.\n",
    "\n",
    " - In extremely randomized forests, more randomness is introduced, which further reduces the variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19f845",
   "metadata": {},
   "source": [
    "### Boosting Methods\n",
    "Boosting Methods combine several weak models to create a improvised ensemble.\n",
    "\n",
    "sklearn.ensemble also provides the following boosting algorithms:\n",
    " - AdaBoostClassifier\n",
    " - GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412d25a",
   "metadata": {},
   "source": [
    "### Demo of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9130f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 1.0\n",
      "Accuracy of Test Data : 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                   stratify=cancer.target, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier = rf_classifier.fit(X_train,Y_train)\n",
    "print('Accuracy of Train Data :', rf_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', rf_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780900e",
   "metadata": {},
   "source": [
    "## Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8219a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n",
      "(127, 13)\n",
      "0.9805545439239387\n",
      "0.88608530301534\n",
      "[19.17   9.887]\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(100)\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target,random_state=30)\n",
    "\n",
    "print(X_train.shape)\n",
    "print( X_test.shape)\n",
    "\n",
    "# Task 2\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg  = RandomForestRegressor()\n",
    "rf_reg = rf_reg.fit(X_train,Y_train)\n",
    "print(rf_reg.score(X_train,Y_train))\n",
    "print(rf_reg.score(X_test,Y_test))\n",
    "print(rf_reg.predict(X_test[:2]))\n",
    "\n",
    "# Task 3\n",
    "\n",
    "best_acc = 0\n",
    "best_r = 0\n",
    "f = 0\n",
    "best_e = [50,100,200]\n",
    "for r in range(3,6):\n",
    "    for e in best_e:\n",
    "        rf_reg = RandomForestRegressor(max_depth=r, n_estimators=e)\n",
    "        rf_reg = rf_reg.fit(X_train,Y_train)\n",
    "        acc = rf_reg.score(X_test,Y_test)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_r = r\n",
    "            f = e\n",
    "print((r,f))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb65724",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build multiple Random Forest Regressor models on X_train data and Y_train labels with max_depth parameter value changing from 3 to 5 and also setting n_estimators to one of 50,100,200 values\n",
    "evaluate each model accuracy on testing dataset Hint make use of for loop\n",
    "print the max_depth and n_estimators values of the model with highest accuracy use boston data from sklearn\n",
    "Note: Print the parameter values in the form of tuple (a,b), a refers to max_depth value and b refers to n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada13e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for max_depth=3, n_estimators=50: 0.8298212292045748\n",
      "Accuracy for max_depth=3, n_estimators=100: 0.8269148884724211\n",
      "Accuracy for max_depth=3, n_estimators=200: 0.8170980159281125\n",
      "Accuracy for max_depth=4, n_estimators=50: 0.8674612615279816\n",
      "Accuracy for max_depth=4, n_estimators=100: 0.8601555677593314\n",
      "Accuracy for max_depth=4, n_estimators=200: 0.8541638877397426\n",
      "Accuracy for max_depth=5, n_estimators=50: 0.8852805002144195\n",
      "Accuracy for max_depth=5, n_estimators=100: 0.8785132418642353\n",
      "Accuracy for max_depth=5, n_estimators=200: 0.8704427213759016\n",
      "Best accuracy: 0.8852805002144195 with max_depth=5, n_estimators=50\n",
      "Best accuracy: 0.8852805002144195 with (max_depth=5, n_estimators=50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load Boston dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the max_depth and n_estimators values to iterate over\n",
    "max_depths = [3, 4, 5]\n",
    "n_estimators_values = [50, 100, 200]\n",
    "\n",
    "# Initialize variables to store the best accuracy and the corresponding max_depth and n_estimators values\n",
    "best_accuracy = 0\n",
    "best_max_depth = 0\n",
    "best_n_estimators = 0\n",
    "\n",
    "# Loop over the max_depth and n_estimators values to build and evaluate the Random Forest Regressor models\n",
    "for max_depth in max_depths:\n",
    "    for n_estimators in n_estimators_values:\n",
    "        \n",
    "        # Build the Random Forest Regressor model with the current max_depth and n_estimators values\n",
    "        rfr = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=42)\n",
    "        rfr.fit(X_train, Y_train)\n",
    "        \n",
    "        # Evaluate the accuracy of the model on the testing dataset\n",
    "        accuracy = rfr.score(X_test, Y_test)\n",
    "        print(f\"Accuracy for max_depth={max_depth}, n_estimators={n_estimators}: {accuracy}\")\n",
    "        \n",
    "        # Update the best_accuracy and corresponding max_depth and n_estimators values if the current model has higher accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_max_depth = max_depth\n",
    "            best_n_estimators = n_estimators\n",
    "\n",
    "# Print the max_depth and n_estimators values of the model with highest accuracy\n",
    "print(f\"Best accuracy: {best_accuracy} with max_depth={best_max_depth}, n_estimators={best_n_estimators}\")\n",
    "# Print the max_depth and n_estimators values of the model with highest accuracy as a tuple\n",
    "print(f\"Best accuracy: {best_accuracy} with (max_depth={best_max_depth}, n_estimators={best_n_estimators})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba6273",
   "metadata": {},
   "source": [
    "## Understanding SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff241dbc",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) separates data points based on decision planes, which separates objects belonging to different classes in a higher dimensional space.\n",
    "\n",
    "   - SVM algorithm uses the best suitable kernel, which is capable of separating data points into two or more classes.\n",
    "   - Commonly used kernels are:\n",
    "\n",
    " - linear\n",
    " - polynomial\n",
    " - rbf\n",
    " - sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb1603",
   "metadata": {},
   "source": [
    "### Support Vector Classification\n",
    "\n",
    "scikit-learn provides the following three utilities for performing Support Vector Classification.\n",
    "\n",
    " - SVC,\n",
    " - NuSVC: Same as SVC but uses a parameter to control the number of support vectors.\n",
    " - LinearSVC: Similar to SVC with parameter kernel taking linear value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6205c",
   "metadata": {},
   "source": [
    "### Support Vector Regression\n",
    "scikit-learn provides the following three utilities for performing Support Vector Regression.\n",
    "\n",
    " - SVR\n",
    " - NuSVR\n",
    " - LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9eda18",
   "metadata": {},
   "source": [
    "### Advantages of SVMs\n",
    " - SVM can distinguish the classes in a higher dimensional space.\n",
    "\n",
    " - SVM algorithms are memory efficient.\n",
    "\n",
    " - SVMs are versatile, and a different kernel can be used by a decision function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da4bc5",
   "metadata": {},
   "source": [
    "### Disadvantages of SVMs\n",
    " - SVMs do not perform well on high dimensional data with many samples.\n",
    "\n",
    " - SVMs work better only with Preprocessed data.\n",
    "\n",
    " - They are harder to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f373c8",
   "metadata": {},
   "source": [
    "### Demo of Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa32af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9178403755868545\n",
      "Accuracy of Test Data : 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                   stratify=cancer.target, random_state=42)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "svm_classifier = svm_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9b8ec",
   "metadata": {},
   "source": [
    "### Improving Accuracy Using Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92ec7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9178403755868545\n",
      "Accuracy of Test Data : 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(cancer.data)\n",
    "cancer_standardized = standardizer.transform(cancer.data)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier = svm_classifier.fit(X_train,Y_train)\n",
    "print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d9828",
   "metadata": {},
   "source": [
    "### Viewing the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc747cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89        53\n",
      "           1       0.91      0.98      0.94        90\n",
      "\n",
      "    accuracy                           0.92       143\n",
      "   macro avg       0.93      0.90      0.92       143\n",
      "weighted avg       0.93      0.92      0.92       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = svm_classifier.predict(X_test)\n",
    "print(\"Classification report : \\n\", metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfc799",
   "metadata": {},
   "source": [
    "## Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c368c6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "0.9822222222222222\n",
      "0.975\n"
     ]
    }
   ],
   "source": [
    "# task 1\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data, digits.target,\n",
    "                                                   stratify=digits.target, random_state=30)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# task 2\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC()\n",
    "svm_clf = svm_clf.fit(X_train,Y_train)\n",
    "print(svm_clf.score(X_test,Y_test))\n",
    "\n",
    "# task 3\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "digits_standardized = scaler.fit_transform(digits.data)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits_standardized, digits.target, test_size=0.2, random_state=30, stratify=digits.target)\n",
    "\n",
    "svm_clf2 = SVC()\n",
    "svm_clf2 = svm_clf2.fit(X_train,Y_train)\n",
    "print(svm_clf2.score(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827da485",
   "metadata": {},
   "source": [
    "## Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabc168",
   "metadata": {},
   "source": [
    " - Clustering is one of the unsupervised learning technique.\n",
    "\n",
    " - The technique is typically used to group data points into clusters based on a specific algorithm.\n",
    "\n",
    " - Major clustering algorithms that can be implemented using scikit-learn are:\n",
    "\n",
    "  - K-means Clustering\n",
    "  - Agglomerative clustering\n",
    "  - DBSCAN clustering\n",
    "  - Mean-shift clustering\n",
    "  - Affinity propagation\n",
    "  - Spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77345a7d",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "In K-means Clustering entire data set is grouped into k clusters.\n",
    "\n",
    "Steps involved are:\n",
    "\n",
    " - k centroids are chosen randomly.\n",
    " - The distance of each data point from k centroids is calculated. A data point is assigned to the nearest cluster.\n",
    " - Centroids of k clusters are recomputed.\n",
    " - The above steps are iterated till the number of data points a cluster reach convergence.\n",
    " - KMeans from sklearn.cluster can be used for K-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b6ddf",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Clustering\n",
    "Agglomerative Hierarchical Clustering is a bottom-up approach.\n",
    "\n",
    "Steps involved are:\n",
    "\n",
    " - Each data point is treated as a single cluster at the beginning.\n",
    "\n",
    " - The distance between each cluster is computed, and the two nearest clusters are merged together.\n",
    "\n",
    " - The above step is iterated till a single cluster is formed.\n",
    "\n",
    " - AgglomerativeClustering from sklearn.cluster can be used for achieving this.\n",
    "\n",
    " - Merging of two clusters can be any of the following linkage type: ward, complete or average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9565975",
   "metadata": {},
   "source": [
    "### Mean Shift Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53301eb",
   "metadata": {},
   "source": [
    "Mean Shift Clustering aims at discovering dense areas.\n",
    "\n",
    "Steps Involved:\n",
    "\n",
    " - Identify blob areas with randomly guessed centroids.\n",
    " - Calculate the centroid of each blob area and shift to a new one, if there is a difference.\n",
    " - Repeat the above step till the centroids converge.\n",
    " - make_blobs from sklearn.cluster can be used to initialize the blob areas. MeanShift from sklearn.cluster can be used to perform Mean Shift clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3f06c",
   "metadata": {},
   "source": [
    "### Affinity Propagation\n",
    "Affinity Propagation generates clusters by passing messages between pairs of data points, until convergence.\n",
    "\n",
    "AffinityPropagation class from sklearn.cluster can be used.\n",
    "\n",
    " - The above class can be controlled with two major parameters:\n",
    "\n",
    " - preference: It controls the number of exemplars to be chosen by the algorithm.\n",
    " - damping: It controls numerical oscillations while updating messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba74525",
   "metadata": {},
   "source": [
    "### Spectral Clustering\n",
    "Spectral Clustering is ideal to cluster data that is connected, and may not be in a compact space.\n",
    "\n",
    "In general, the following steps are followed:\n",
    "     - Build an affinity matrix of data points.\n",
    "     - Embed data points in a lower dimensional space.\n",
    "     - Use a clustering method like k-means to partition the points on lower dimensional space\n",
    "spectral_clustering from sklearn.cluster can be used for achieving this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d4b53",
   "metadata": {},
   "source": [
    "### Demo of KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29cf927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=2)\n",
    "\n",
    "kmeans_cluster = kmeans_cluster.fit(X_train) \n",
    "\n",
    "kmeans_cluster.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c2f61",
   "metadata": {},
   "source": [
    "### Evaluating a Clustering algorithm\n",
    "A clustering algorithm is majorly evaluated using the following scores:\n",
    "\n",
    " - Homogeneity: Evaluates if each cluster contains only members of a single class.\n",
    "\n",
    " - Completeness: All members of a given class are assigned to the same cluster.\n",
    "\n",
    " - V-measure: Harmonic mean of Homogeneity and Completeness.\n",
    "\n",
    " - Adjusted Rand index: Measures similarity of two assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35c0b6",
   "metadata": {},
   "source": [
    "### Evaluation with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ae018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.homogeneity_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.completeness_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.v_measure_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.adjusted_rand_score(kmeans_cluster.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523681b",
   "metadata": {},
   "source": [
    "## Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7dcd613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7514854021988339\n",
      "0.7608008469718723\n",
      "0.9149410296693684\n"
     ]
    }
   ],
   "source": [
    "# task 1\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_score\n",
    "\n",
    "# load iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# cluster iris data set into 3 clusters using k-means with default parameters\n",
    "km_cls = KMeans(n_clusters=3).fit(iris.data)\n",
    "\n",
    "# determine the homogeneity score of the model and print it\n",
    "score = homogeneity_score(iris.target, km_cls.labels_)\n",
    "print(score)\n",
    "\n",
    "# task 2\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg_cls = AgglomerativeClustering(n_clusters=3).fit(iris.data)\n",
    "score = homogeneity_score(iris.target, agg_cls.labels_)\n",
    "print(score)\n",
    "\n",
    "# task 3\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "af_cls = AffinityPropagation().fit(iris.data)\n",
    "score = homogeneity_score(iris.target, af_cls.labels_)\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002dafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import three modules sklearn.datasets, sklearn.cluster and sklearn.metrics\n",
    "load popular iris dataset from sklearn.datasets module and assign it to variable iris\n",
    "cluster iris.data set into 3 clusters using k-means with default parameters Name the model as km_cls\n",
    "Hint : import required utility from sklearn.cluster\n",
    "Determine the homogeneity score of the model and print it\n",
    "Hint : import required utility from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215fb26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity score of the KMeans model: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_score\n",
    "\n",
    "# load iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# cluster iris data set into 3 clusters using k-means with default parameters\n",
    "km_cls = KMeans(n_clusters=3).fit(iris.data)\n",
    "\n",
    "# determine the homogeneity score of the model and print it\n",
    "score = homogeneity_score(iris.target, km_cls.labels_)\n",
    "print(\"Homogeneity score of the KMeans model: %.2f\" % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6f47a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a55bd571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799164b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
